# Clusters as features

This demo shows how some perverse data can be untangled using k-means features. The data in question consists of interlocking spirals.

The additional features here are generated by clustering the data and adding a factor that indicates which cluster is closest to a datapoint. The clusters are generated either by looking just at the original data, or by looking at the data and the target variable. The latter process is called hinting. Hinting can be valuable if the data is uniformly spread out. The data used in this example isn't very uniform because there is a bit of a gap between the spirals and thus hinting is much less useful than it might otherwise be.

The figures here show the clustering itself (figure 1) and the performance advantage of cluster features (figure 2) when used with simple logistic regression.

## Caveats

Some of the power of this method comes from the fact that we are using a higher dimensional space when we have the clusters. This can make linear classifiers like logistic regression work better. Simply spattering cluster centroids around at random, for instance, would give us similar gains (though hopefully not quite as good). Not all dimensionality increases are equally good, however. Random linear projection into a higher dimensional space, for instance, would make no difference whatsoever (the data would still be planar). 
